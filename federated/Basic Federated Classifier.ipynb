{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic federated classifier with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is copyright 2018 <a href='https://comind.org/'>coMind</a>. Licensed under the Apache License, Version 2.0; you may not use this code except in compliance with the License. You may obtain a copy of the <a href='http://www.apache.org/licenses/LICENSE-2.0'>License</a>.\n",
    "\n",
    "Join the <a href='https://comindorg.slack.com/join/shared_invite/enQtNDMxMzc0NDA5OTEwLWIyZTg5MTg1MTM4NjhiNDM4YTU1OTI1NTgwY2NkNzZjYWY1NmI0ZjIyNWJiMTNkZmRhZDg2Nzc3YTYyNGQzM2I'>conversation</a> at Slack.\n",
    "\n",
    "This a series of three tutorials you are in the last one: \n",
    "* [Basic Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Classifier.ipynb)\n",
    "* [Basic Distributed Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Distributed%20Classifier.ipynb)\n",
    "* [Basic Federated Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Federated%20Classifier.ipynb)\n",
    "\n",
    "In this tutorial we will see how to train a model using federated averaging.\n",
    "\n",
    "To begin a brief explanation of what it means to train using federated averaging with respect to training using a SyncReplicasOptimizer.\n",
    "\n",
    "In the previous tutorial, we explained that with SyncReplicasOptimizer each worker generated a gradient for its weights and wrote it to the parameter server. The chief read those gradients (including its own), it averaged them and updated the shared model.\n",
    "\n",
    "This time each worker will be updating its weights locally, as if it were the only one training. Every certain number of steps it will send its weights (not the gradients, but the weights themselves) to the parameter server. The chief will read the weights from there, it will average and write them again to the parameter server so that all the workers can overwrite theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire first part of the code is the same as the distributed classifier tutorial.\n",
    "\n",
    "Two differences only:\n",
    "\n",
    "- This time we also import __federated_average_optimizer__, the library with which we can federalize learning.\n",
    "- On the other hand we define the variable __INTERVAL_STEPS__. Every how many steps we will perform the average of the weights. Put another way, how many steps will each worker make in local before writing their weights in the parameter server and overwriting them with the average that the chief has made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import federated_averaging_optimizer\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"task_index\", None,\n",
    "                     \"Worker task index, should be >= 0. task_index=0 is \"\n",
    "                     \"the master worker task that performs the variable \"\n",
    "                     \"initialization \")\n",
    "flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"job_name\", None, \"job name: worker or ps\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "INTERVAL_STEPS = 10\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "if FLAGS.job_name is None or FLAGS.job_name == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `job_name`\")\n",
    "if FLAGS.task_index is None or FLAGS.task_index == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `task_index`\")\n",
    "\n",
    "if FLAGS.task_index == 0:\n",
    "    print('--- GPU Disabled ---')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "#Construct the cluster and start the server\n",
    "ps_spec = FLAGS.ps_hosts.split(\",\")\n",
    "worker_spec = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "# Get the number of workers.\n",
    "num_workers = len(worker_spec)\n",
    "print('{} workers defined'.format(num_workers))\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})\n",
    "\n",
    "server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n",
    "if FLAGS.job_name == \"ps\":\n",
    "    print('--- Parameter Server Ready ---')\n",
    "    server.join()\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print('Data loaded')\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = np.split(train_images, num_workers)[FLAGS.task_index]\n",
    "train_labels = np.split(train_labels, num_workers)[FLAGS.task_index]\n",
    "print('Local dataset size: {}'.format(train_images.shape[0]))\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "is_chief = (FLAGS.task_index == 0)\n",
    "\n",
    "checkpoint_dir='logs_dir/federated_worker_{}/{}'.format(FLAGS.task_index, time())\n",
    "print('Checkpoint directory: ' + checkpoint_dir)\n",
    "\n",
    "worker_device = \"/job:worker/task:%d\" % FLAGS.task_index\n",
    "print('Worker device: ' + worker_device + ' - is_chief: {}'.format(is_chief))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin the definition of the graph in the same way as it was done in the basic classifier, we explicitly place every operation in the local worker. The rest is fairly standard until we reach the definition of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(worker_device)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    with tf.name_scope('dataset'), tf.device('/cpu:0'):\n",
    "        images_placeholder = tf.placeholder(train_images.dtype, [None, train_images.shape[1], train_images.shape[2]], \n",
    "                                            name='images_placeholder')\n",
    "        labels_placeholder = tf.placeholder(train_labels.dtype, [None], name='labels_placeholder')\n",
    "        batch_size = tf.placeholder(tf.int64, name='batch_size')\n",
    "        shuffle_size = tf.placeholder(tf.int64, name='shuffle_size')\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images_placeholder, labels_placeholder))\n",
    "        dataset = dataset.shuffle(shuffle_size, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat(EPOCHS)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n",
    "        X, y = iterator.get_next()\n",
    "\n",
    "    flatten_layer = tf.layers.flatten(X, name='flatten')\n",
    "\n",
    "    dense_layer = tf.layers.dense(flatten_layer, 128, activation=tf.nn.relu, name='relu')\n",
    "\n",
    "    predictions = tf.layers.dense(dense_layer, 10, activation=tf.nn.softmax, name='softmax')\n",
    "\n",
    "    summary_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(keras.losses.sparse_categorical_crossentropy(y, predictions))\n",
    "        loss_averages_op = summary_averages.apply([loss])\n",
    "        tf.summary.scalar('cross_entropy', summary_averages.average(loss))\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.cast(y, tf.int64))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy_metric')\n",
    "        accuracy_averages_op = summary_averages.apply([accuracy])\n",
    "        tf.summary.scalar('accuracy', summary_averages.average(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the __replica_device_setter__ in the distributed learning to automatically choose in which device to place each defined op. Here we create it just to pass it as an argument to the custom optimizer that we have created to contain the logic of the federated averaging.\n",
    "\n",
    "This custom optimizer will use the __replica_device_setter__ to place a copy of each trainable variable in the ps, this new variables will store the averaged values of all the local models.\n",
    "\n",
    "Once this optimizer has been defined, we create the training operation and a, in the same way as we did with SyncReplicasOptimizer, a hook that will run inside the MonitoredTrainingSession, which handles the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with tf.name_scope('train'):\n",
    "        device_setter = tf.train.replica_device_setter(worker_device=worker_device, cluster=cluster)\n",
    "        optimizer = federated_averaging_optimizer.FederatedAveragingOptimizer(\n",
    "            tf.train.AdamOptimizer(np.sqrt(num_workers) * 0.001), \n",
    "            replicas_to_aggregate=num_workers, interval_steps=INTERVAL_STEPS, is_chief=is_chief, \n",
    "            device_setter=device_setter)\n",
    "        with tf.control_dependencies([loss_averages_op, accuracy_averages_op]):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        model_average_hook = optimizer.make_session_run_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep defining our hooks as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = int(train_images.shape[0] / BATCH_SIZE)\n",
    "last_step = int(n_batches * EPOCHS)\n",
    "\n",
    "print('Graph definition finished')\n",
    "\n",
    "sess_config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=False,\n",
    "    operation_timeout_in_ms=20000,\n",
    "    device_filters=[\"/job:ps\",\n",
    "    \"/job:worker/task:%d\" % FLAGS.task_index])\n",
    "\n",
    "print('Training {} batches...'.format(last_step))\n",
    "\n",
    "class _LoggerHook(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self._total_loss = 0\n",
    "        self._total_acc = 0\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        return tf.train.SessionRunArgs([loss, accuracy, global_step])\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        loss_value, acc_value, step_value = run_values.results\n",
    "        self._total_loss += loss_value\n",
    "        self._total_acc += acc_value\n",
    "        if (step_value + 1) % n_batches == 0 and not step_value == 0:\n",
    "            print(\"Epoch {}/{} - loss: {:.4f} - acc: {:.4f}\".format(\n",
    "                int(step_value / n_batches) + 1, EPOCHS, self._total_loss / n_batches, self._total_acc / n_batches))\n",
    "            self._total_loss = 0\n",
    "            self._total_acc = 0\n",
    "\n",
    "class _InitHook(tf.train.SessionRunHook):\n",
    "    def after_create_session(self, session, coord):\n",
    "        session.run(dataset_init_op, feed_dict={\n",
    "            images_placeholder: train_images, labels_placeholder: train_labels, \n",
    "            batch_size: BATCH_SIZE, shuffle_size: train_images.shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared variables generated within the custom optimizer get their initialized value from their corresponding trainable variables in the local worker. Therefore their initialization ops will be unavailable out of this session even if we try to restore a saved checkpoint.\n",
    "\n",
    "We need to define a custom saver which ignores this shared variables. In this case, we only save the trainable_variables ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SaverHook(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self._saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        return tf.train.SessionRunArgs(global_step)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        step_value = run_values.results\n",
    "        if step_value % n_batches == 0 and not step_value == 0:\n",
    "            self._saver.save(run_context.session, checkpoint_dir+'/model.ckpt', step_value)\n",
    "\n",
    "    def end(self, session):\n",
    "        self._saver.save(session, checkpoint_dir+'/model.ckpt', session.run(global_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the training session is standard. Notice the new hooks that we have added to the hook lists.\n",
    "\n",
    "WARNING! Do not define a chief worker. We need each worker to initialize their local session and train on its own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('monitored_session'):\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "            master=server.target,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            hooks=[_LoggerHook(), _InitHook(), _SaverHook(), model_average_hook],\n",
    "            config=sess_config,\n",
    "            stop_grace_period_secs=10,\n",
    "            save_checkpoint_secs=None) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_chief:\n",
    "    print('--- Begin Evaluation ---')\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path + '.meta', clear_devices=True)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Model restored')\n",
    "        graph = tf.get_default_graph()\n",
    "        images_placeholder = graph.get_tensor_by_name('dataset/images_placeholder:0')\n",
    "        labels_placeholder = graph.get_tensor_by_name('dataset/labels_placeholder:0')\n",
    "        batch_size = graph.get_tensor_by_name('dataset/batch_size:0')\n",
    "        accuracy = graph.get_tensor_by_name('accuracy/accuracy_metric:0')\n",
    "        predictions = graph.get_tensor_by_name('softmax/BiasAdd:0')\n",
    "        dataset_init_op = graph.get_operation_by_name('dataset/dataset_init')\n",
    "        sess.run(dataset_init_op, feed_dict={\n",
    "            images_placeholder: test_images, labels_placeholder: test_labels, \n",
    "            batch_size: test_images.shape[0], shuffle_size: 1})\n",
    "        print('Test accuracy: {:4f}'.format(sess.run(accuracy)))\n",
    "        predicted = sess.run(predictions)\n",
    "\n",
    "    # Plot the first 25 test images, their predicted label, and the true label\n",
    "    # Color correct predictions in green, incorrect predictions in red\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "        predicted_label = np.argmax(predicted[i])\n",
    "        true_label = test_labels[i]\n",
    "        if predicted_label == true_label:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.xlabel(\"{} ({})\".format(class_names[predicted_label],\n",
    "                                    class_names[true_label]),\n",
    "                                    color=color)\n",
    "\n",
    "    plt.show(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
