{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic federated classifier with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is copyright 2018 <a href='https://comind.org/'>coMind</a>. Licensed under the Apache License, Version 2.0; you may not use this code except in compliance with the License. You may obtain a copy of the <a href='http://www.apache.org/licenses/LICENSE-2.0'>License</a>.\n",
    "\n",
    "Join the <a href='https://comindorg.slack.com/join/shared_invite/enQtNDMxMzc0NDA5OTEwLWIyZTg5MTg1MTM4NjhiNDM4YTU1OTI1NTgwY2NkNzZjYWY1NmI0ZjIyNWJiMTNkZmRhZDg2Nzc3YTYyNGQzM2I'>conversation</a> at Slack.\n",
    "\n",
    "This a series of three tutorials you are in the last one: \n",
    "* [Basic Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Classifier.ipynb)\n",
    "* [Basic Distributed Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Distributed%20Classifier.ipynb)\n",
    "* [Basic Federated Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Federated%20Classifier.ipynb)\n",
    "\n",
    "In this tutorial we will see how to train a model using federated averaging.\n",
    "\n",
    "To begin a brief explanation of what it means to train using federated averaging with respect to training using a SyncReplicasOptimizer.\n",
    "\n",
    "In the previous tutorial, we explained that with SyncReplicasOptimizer each worker generated a gradient for its weights and wrote it to the parameter server. The chief read those gradients (including its own), it averaged them and updated the shared model.\n",
    "\n",
    "This time each worker will be updating its weights locally, as if it were the only one training. Every certain number of steps it will send its weights (not the gradients, but the weights themselves) to the parameter server. The chief will read the weights from there, it will average and write them again to the parameter server so that all the workers can overwrite theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire first part of the code is the same as the distributed classifier tutorial.\n",
    "\n",
    "Two differences only:\n",
    "\n",
    "- This time we also import __federated_average_optimizer__, the library with which we can federalize learning.\n",
    "- On the other hand we define the variable __INTERVAL_STEPS__. Every how many steps we will perform the average of the weights. Put another way, how many steps will each worker make in local before writing their weights in the parameter server and overwriting them with the average that the chief has made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2 workers defined\n",
      "Data loaded\n",
      "Local dataset size: 30000\n",
      "Checkpoint directory: logs_dir/federated_worker_1/1582488396.3915372\n",
      "Worker device: /job:worker/task:1 - is_chief: False\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import federated_averaging_optimizer\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"task_index\", None,\n",
    "                     \"Worker task index, should be >= 0. task_index=0 is \"\n",
    "                     \"the master worker task that performs the variable \"\n",
    "                     \"initialization \")\n",
    "flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"job_name\", None, \"job name: worker or ps\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "INTERVAL_STEPS = 3\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.job_name = \"worker\"\n",
    "FLAGS.task_index = 1\n",
    "\n",
    "if FLAGS.job_name is None or FLAGS.job_name == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `job_name`\")\n",
    "if FLAGS.task_index is None or FLAGS.task_index == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `task_index`\")\n",
    "\n",
    "if FLAGS.task_index == 0:\n",
    "    print('--- GPU Disabled ---')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "#Construct the cluster and start the server\n",
    "ps_spec = FLAGS.ps_hosts.split(\",\")\n",
    "worker_spec = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "# Get the number of workers.\n",
    "num_workers = len(worker_spec)\n",
    "print('{} workers defined'.format(num_workers))\n",
    "\n",
    "cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})\n",
    "\n",
    "server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n",
    "if FLAGS.job_name == \"ps\":\n",
    "    print('--- Parameter Server Ready ---')\n",
    "    server.join()\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print('Data loaded')\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = np.split(train_images, num_workers)[FLAGS.task_index]\n",
    "train_labels = np.split(train_labels, num_workers)[FLAGS.task_index]\n",
    "print('Local dataset size: {}'.format(train_images.shape[0]))\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "is_chief = (FLAGS.task_index == 0)\n",
    "\n",
    "checkpoint_dir='logs_dir/federated_worker_{}/{}'.format(FLAGS.task_index, time())\n",
    "print('Checkpoint directory: ' + checkpoint_dir)\n",
    "\n",
    "worker_device = \"/job:worker/task:%d\" % FLAGS.task_index\n",
    "print('Worker device: ' + worker_device + ' - is_chief: {}'.format(is_chief))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin the definition of the graph in the same way as it was done in the basic classifier, we explicitly place every operation in the local worker. The rest is fairly standard until we reach the definition of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f788e86f922b>:15: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "WARNING:tensorflow:From <ipython-input-2-f788e86f922b>:15: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
      "WARNING:tensorflow:From <ipython-input-2-f788e86f922b>:19: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-f788e86f922b>:21: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(worker_device):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    with tf.name_scope('dataset'), tf.device('/cpu:0'):\n",
    "        images_placeholder = tf.placeholder(train_images.dtype, [None, train_images.shape[1], train_images.shape[2]], \n",
    "                                            name='images_placeholder')\n",
    "        labels_placeholder = tf.placeholder(train_labels.dtype, [None], name='labels_placeholder')\n",
    "        batch_size = tf.placeholder(tf.int64, name='batch_size')\n",
    "        shuffle_size = tf.placeholder(tf.int64, name='shuffle_size')\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images_placeholder, labels_placeholder))\n",
    "        dataset = dataset.shuffle(shuffle_size, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat(EPOCHS)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n",
    "        X, y = iterator.get_next()\n",
    "\n",
    "    flatten_layer = tf.layers.flatten(X, name='flatten')\n",
    "\n",
    "    dense_layer = tf.layers.dense(flatten_layer, 128, activation=tf.nn.relu, name='relu')\n",
    "\n",
    "    predictions = tf.layers.dense(dense_layer, 10, activation=tf.nn.softmax, name='softmax')\n",
    "\n",
    "    summary_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(keras.losses.sparse_categorical_crossentropy(y, predictions))\n",
    "        loss_averages_op = summary_averages.apply([loss])\n",
    "        tf.summary.scalar('cross_entropy', summary_averages.average(loss))\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.cast(y, tf.int64))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy_metric')\n",
    "        accuracy_averages_op = summary_averages.apply([accuracy])\n",
    "        tf.summary.scalar('accuracy', summary_averages.average(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the __replica_device_setter__ in the distributed learning to automatically choose in which device to place each defined op. Here we create it just to pass it as an argument to the custom optimizer that we have created to contain the logic of the federated averaging.\n",
    "\n",
    "This custom optimizer will use the __replica_device_setter__ to place a copy of each trainable variable in the ps, this new variables will store the averaged values of all the local models.\n",
    "\n",
    "Once this optimizer has been defined, we create the training operation and a, in the same way as we did with SyncReplicasOptimizer, a hook that will run inside the MonitoredTrainingSession, which handles the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:FedAvgV4: replicas_to_aggregate=2; total_num_replicas=2\n",
      "WARNING:tensorflow:From /home/absinthe/Project/colocate/federated-averaging-tutorials/federated/federated_averaging_optimizer.py:195: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "    with tf.name_scope('train'):\n",
    "        device_setter = tf.train.replica_device_setter(worker_device=worker_device, cluster=cluster)\n",
    "        optimizer = federated_averaging_optimizer.FederatedAveragingOptimizer(\n",
    "            tf.train.AdamOptimizer(np.sqrt(num_workers) * 0.001), \n",
    "            replicas_to_aggregate=num_workers, interval_steps=INTERVAL_STEPS, is_chief=is_chief, \n",
    "            device_setter=device_setter)\n",
    "        with tf.control_dependencies([loss_averages_op, accuracy_averages_op]):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        model_average_hook = optimizer.make_session_run_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep defining our hooks as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph definition finished\n",
      "Training 28110 batches...\n"
     ]
    }
   ],
   "source": [
    "n_batches = int(train_images.shape[0] / BATCH_SIZE)\n",
    "last_step = int(n_batches * EPOCHS)\n",
    "\n",
    "print('Graph definition finished')\n",
    "\n",
    "sess_config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=False,\n",
    "    operation_timeout_in_ms=20000,\n",
    "    device_filters=[\"/job:ps\",\n",
    "    \"/job:worker/task:%d\" % FLAGS.task_index])\n",
    "\n",
    "print('Training {} batches...'.format(last_step))\n",
    "\n",
    "class _LoggerHook(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self._total_loss = 0\n",
    "        self._total_acc = 0\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        return tf.train.SessionRunArgs([loss, accuracy, global_step])\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        loss_value, acc_value, step_value = run_values.results\n",
    "        self._total_loss += loss_value\n",
    "        self._total_acc += acc_value\n",
    "        if (step_value + 1) % n_batches == 0 and not step_value == 0:\n",
    "            print(\"Epoch {}/{} - loss: {:.4f} - acc: {:.4f}\".format(\n",
    "                int(step_value / n_batches) + 1, EPOCHS, self._total_loss / n_batches, self._total_acc / n_batches))\n",
    "            self._total_loss = 0\n",
    "            self._total_acc = 0\n",
    "\n",
    "class _InitHook(tf.train.SessionRunHook):\n",
    "    def after_create_session(self, session, coord):\n",
    "        session.run(dataset_init_op, feed_dict={\n",
    "            images_placeholder: train_images, labels_placeholder: train_labels, \n",
    "            batch_size: BATCH_SIZE, shuffle_size: train_images.shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared variables generated within the custom optimizer get their initialized value from their corresponding trainable variables in the local worker. Therefore their initialization ops will be unavailable out of this session even if we try to restore a saved checkpoint.\n",
    "\n",
    "We need to define a custom saver which ignores this shared variables. In this case, we only save the trainable_variables ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SaverHook(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self._saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        return tf.train.SessionRunArgs(global_step)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        step_value = run_values.results\n",
    "        if step_value % n_batches == 0 and not step_value == 0:\n",
    "            self._saver.save(run_context.session, checkpoint_dir+'/model.ckpt', step_value)\n",
    "\n",
    "    def end(self, session):\n",
    "        self._saver.save(session, checkpoint_dir+'/model.ckpt', session.run(global_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the training session is standard. Notice the new hooks that we have added to the hook lists.\n",
    "\n",
    "WARNING! Do not define a chief worker. We need each worker to initialize their local session and train on its own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 31 vs previous value: 31. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 46 vs previous value: 46. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 69 vs previous value: 69. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 93 vs previous value: 93. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 18.9067\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 103 vs previous value: 103. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 102.081\n",
      "INFO:tensorflow:global_step/sec: 119.1\n",
      "INFO:tensorflow:global_step/sec: 138.058\n",
      "INFO:tensorflow:global_step/sec: 113.456\n",
      "INFO:tensorflow:global_step/sec: 108.069\n",
      "INFO:tensorflow:global_step/sec: 102.678\n",
      "INFO:tensorflow:global_step/sec: 111.751\n",
      "INFO:tensorflow:global_step/sec: 118.034\n",
      "Epoch 1/30 - loss: 0.6187 - acc: 0.7899\n",
      "INFO:tensorflow:global_step/sec: 76.6134\n",
      "INFO:tensorflow:global_step/sec: 102.196\n",
      "INFO:tensorflow:global_step/sec: 106.633\n",
      "INFO:tensorflow:global_step/sec: 134.314\n",
      "INFO:tensorflow:global_step/sec: 135.112\n",
      "INFO:tensorflow:global_step/sec: 139.726\n",
      "INFO:tensorflow:global_step/sec: 139.437\n",
      "INFO:tensorflow:global_step/sec: 138.567\n",
      "INFO:tensorflow:global_step/sec: 138.098\n",
      "Epoch 2/30 - loss: 0.4427 - acc: 0.8463\n",
      "INFO:tensorflow:global_step/sec: 103.736\n",
      "INFO:tensorflow:global_step/sec: 135.953\n",
      "INFO:tensorflow:global_step/sec: 137.901\n",
      "INFO:tensorflow:global_step/sec: 127.876\n",
      "INFO:tensorflow:global_step/sec: 122.517\n",
      "INFO:tensorflow:global_step/sec: 126.433\n",
      "INFO:tensorflow:global_step/sec: 137.244\n",
      "INFO:tensorflow:global_step/sec: 124.224\n",
      "INFO:tensorflow:global_step/sec: 120.953\n",
      "INFO:tensorflow:global_step/sec: 137.364\n",
      "Epoch 3/30 - loss: 0.3983 - acc: 0.8590\n",
      "INFO:tensorflow:global_step/sec: 108.233\n",
      "INFO:tensorflow:global_step/sec: 137.89\n",
      "INFO:tensorflow:global_step/sec: 132.8\n",
      "INFO:tensorflow:global_step/sec: 140.048\n",
      "INFO:tensorflow:global_step/sec: 125.961\n",
      "INFO:tensorflow:global_step/sec: 88.3359\n",
      "INFO:tensorflow:global_step/sec: 122.838\n",
      "INFO:tensorflow:global_step/sec: 120.361\n",
      "INFO:tensorflow:global_step/sec: 98.063\n",
      "Epoch 4/30 - loss: 0.3692 - acc: 0.8697\n",
      "INFO:tensorflow:global_step/sec: 65.7438\n",
      "INFO:tensorflow:global_step/sec: 138.121\n",
      "INFO:tensorflow:global_step/sec: 137.68\n",
      "INFO:tensorflow:global_step/sec: 138.155\n",
      "INFO:tensorflow:global_step/sec: 139.957\n",
      "INFO:tensorflow:global_step/sec: 138.433\n",
      "INFO:tensorflow:global_step/sec: 138.107\n",
      "INFO:tensorflow:global_step/sec: 135.751\n",
      "INFO:tensorflow:global_step/sec: 137.27\n",
      "Epoch 5/30 - loss: 0.3486 - acc: 0.8738\n",
      "INFO:tensorflow:global_step/sec: 111.191\n",
      "INFO:tensorflow:global_step/sec: 132.823\n",
      "INFO:tensorflow:global_step/sec: 136.836\n",
      "INFO:tensorflow:global_step/sec: 138.069\n",
      "INFO:tensorflow:global_step/sec: 138.301\n",
      "INFO:tensorflow:global_step/sec: 136.302\n",
      "INFO:tensorflow:global_step/sec: 136.668\n",
      "INFO:tensorflow:global_step/sec: 138.662\n",
      "INFO:tensorflow:global_step/sec: 136.487\n",
      "INFO:tensorflow:global_step/sec: 137.14\n",
      "Epoch 6/30 - loss: 0.3321 - acc: 0.8807\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 105.151\n",
      "INFO:tensorflow:global_step/sec: 132.915\n",
      "INFO:tensorflow:global_step/sec: 136.971\n",
      "INFO:tensorflow:global_step/sec: 135.958\n",
      "INFO:tensorflow:global_step/sec: 136.096\n",
      "INFO:tensorflow:global_step/sec: 137.667\n",
      "INFO:tensorflow:global_step/sec: 126.219\n",
      "INFO:tensorflow:global_step/sec: 135.559\n",
      "INFO:tensorflow:global_step/sec: 135.079\n",
      "Epoch 7/30 - loss: 0.3163 - acc: 0.8862\n",
      "INFO:tensorflow:global_step/sec: 99.9724\n",
      "INFO:tensorflow:global_step/sec: 142.381\n",
      "INFO:tensorflow:global_step/sec: 142.933\n",
      "INFO:tensorflow:global_step/sec: 123.533\n",
      "INFO:tensorflow:global_step/sec: 137.18\n",
      "INFO:tensorflow:global_step/sec: 144.108\n",
      "INFO:tensorflow:global_step/sec: 129.637\n",
      "INFO:tensorflow:global_step/sec: 141.349\n",
      "INFO:tensorflow:global_step/sec: 143.209\n",
      "Epoch 8/30 - loss: 0.3042 - acc: 0.8906\n",
      "INFO:tensorflow:global_step/sec: 124.826\n",
      "INFO:tensorflow:global_step/sec: 132.393\n",
      "INFO:tensorflow:global_step/sec: 145.156\n",
      "INFO:tensorflow:global_step/sec: 144.383\n",
      "INFO:tensorflow:global_step/sec: 141.521\n",
      "INFO:tensorflow:global_step/sec: 143.704\n",
      "INFO:tensorflow:global_step/sec: 142.993\n",
      "INFO:tensorflow:global_step/sec: 143.088\n",
      "INFO:tensorflow:global_step/sec: 143.917\n",
      "INFO:tensorflow:global_step/sec: 142.439\n",
      "Epoch 9/30 - loss: 0.2956 - acc: 0.8929\n",
      "INFO:tensorflow:global_step/sec: 108.981\n",
      "INFO:tensorflow:global_step/sec: 140.981\n",
      "INFO:tensorflow:global_step/sec: 143.785\n",
      "INFO:tensorflow:global_step/sec: 144.527\n",
      "INFO:tensorflow:global_step/sec: 143.031\n",
      "INFO:tensorflow:global_step/sec: 144.817\n",
      "INFO:tensorflow:global_step/sec: 145.094\n",
      "INFO:tensorflow:global_step/sec: 144.24\n",
      "INFO:tensorflow:global_step/sec: 141.058\n",
      "Epoch 10/30 - loss: 0.2830 - acc: 0.8967\n",
      "INFO:tensorflow:global_step/sec: 112.716\n",
      "INFO:tensorflow:global_step/sec: 144.539\n",
      "INFO:tensorflow:global_step/sec: 142.944\n",
      "INFO:tensorflow:global_step/sec: 144.102\n",
      "INFO:tensorflow:global_step/sec: 144.045\n",
      "INFO:tensorflow:global_step/sec: 143.68\n",
      "INFO:tensorflow:global_step/sec: 141.601\n",
      "INFO:tensorflow:global_step/sec: 143.657\n",
      "INFO:tensorflow:global_step/sec: 143.342\n",
      "INFO:tensorflow:global_step/sec: 143.265\n",
      "Epoch 11/30 - loss: 0.2805 - acc: 0.8985\n",
      "INFO:tensorflow:global_step/sec: 110.076\n",
      "INFO:tensorflow:global_step/sec: 142.893\n",
      "INFO:tensorflow:global_step/sec: 144.741\n",
      "INFO:tensorflow:global_step/sec: 142.386\n",
      "INFO:tensorflow:global_step/sec: 143.985\n",
      "INFO:tensorflow:global_step/sec: 144.434\n",
      "INFO:tensorflow:global_step/sec: 143.967\n",
      "INFO:tensorflow:global_step/sec: 143.791\n",
      "INFO:tensorflow:global_step/sec: 143.657\n",
      "Epoch 12/30 - loss: 0.2688 - acc: 0.9037\n",
      "INFO:tensorflow:global_step/sec: 108.809\n",
      "INFO:tensorflow:global_step/sec: 141.353\n",
      "INFO:tensorflow:global_step/sec: 142.84\n",
      "INFO:tensorflow:global_step/sec: 141.885\n",
      "INFO:tensorflow:global_step/sec: 143.359\n",
      "INFO:tensorflow:global_step/sec: 144.282\n",
      "INFO:tensorflow:global_step/sec: 143.747\n",
      "INFO:tensorflow:global_step/sec: 142.892\n",
      "INFO:tensorflow:global_step/sec: 141.689\n",
      "Epoch 13/30 - loss: 0.2629 - acc: 0.9046\n",
      "INFO:tensorflow:global_step/sec: 110.298\n",
      "INFO:tensorflow:global_step/sec: 142.547\n",
      "INFO:tensorflow:global_step/sec: 143.374\n",
      "INFO:tensorflow:global_step/sec: 144.007\n",
      "INFO:tensorflow:global_step/sec: 140.844\n",
      "INFO:tensorflow:global_step/sec: 143.991\n",
      "INFO:tensorflow:global_step/sec: 142.041\n",
      "INFO:tensorflow:global_step/sec: 140.977\n",
      "INFO:tensorflow:global_step/sec: 142.547\n",
      "INFO:tensorflow:global_step/sec: 142.981\n",
      "Epoch 14/30 - loss: 0.2543 - acc: 0.9068\n",
      "INFO:tensorflow:global_step/sec: 108.575\n",
      "INFO:tensorflow:global_step/sec: 145.662\n",
      "INFO:tensorflow:global_step/sec: 143.701\n",
      "INFO:tensorflow:global_step/sec: 139.711\n",
      "INFO:tensorflow:global_step/sec: 143.097\n",
      "INFO:tensorflow:global_step/sec: 144.572\n",
      "INFO:tensorflow:global_step/sec: 141.86\n",
      "INFO:tensorflow:global_step/sec: 142.799\n",
      "INFO:tensorflow:global_step/sec: 142.49\n",
      "Epoch 15/30 - loss: 0.2494 - acc: 0.9087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 107.289\n",
      "INFO:tensorflow:global_step/sec: 140.07\n",
      "INFO:tensorflow:global_step/sec: 142.572\n",
      "INFO:tensorflow:global_step/sec: 143.326\n",
      "INFO:tensorflow:global_step/sec: 145.196\n",
      "INFO:tensorflow:global_step/sec: 143.052\n",
      "INFO:tensorflow:global_step/sec: 143.016\n",
      "INFO:tensorflow:global_step/sec: 144.93\n",
      "INFO:tensorflow:global_step/sec: 142.067\n",
      "Epoch 16/30 - loss: 0.2438 - acc: 0.9104\n",
      "INFO:tensorflow:global_step/sec: 117.956\n",
      "INFO:tensorflow:global_step/sec: 129.889\n",
      "INFO:tensorflow:global_step/sec: 143.766\n",
      "INFO:tensorflow:global_step/sec: 142.746\n",
      "INFO:tensorflow:global_step/sec: 144.913\n",
      "INFO:tensorflow:global_step/sec: 144.155\n",
      "INFO:tensorflow:global_step/sec: 141.644\n",
      "INFO:tensorflow:global_step/sec: 144.179\n",
      "INFO:tensorflow:global_step/sec: 144.013\n",
      "INFO:tensorflow:global_step/sec: 140.651\n",
      "Epoch 17/30 - loss: 0.2387 - acc: 0.9124\n",
      "INFO:tensorflow:global_step/sec: 110.421\n",
      "INFO:tensorflow:global_step/sec: 144.967\n",
      "INFO:tensorflow:global_step/sec: 143.265\n",
      "INFO:tensorflow:global_step/sec: 143.61\n",
      "INFO:tensorflow:global_step/sec: 144.897\n",
      "INFO:tensorflow:global_step/sec: 144.331\n",
      "INFO:tensorflow:global_step/sec: 144.231\n",
      "INFO:tensorflow:global_step/sec: 143.771\n",
      "INFO:tensorflow:global_step/sec: 141.926\n",
      "Epoch 18/30 - loss: 0.2326 - acc: 0.9161\n",
      "INFO:tensorflow:global_step/sec: 107.997\n",
      "INFO:tensorflow:global_step/sec: 99.3945\n",
      "INFO:tensorflow:global_step/sec: 132.963\n",
      "INFO:tensorflow:global_step/sec: 112.614\n",
      "INFO:tensorflow:global_step/sec: 130.707\n",
      "INFO:tensorflow:global_step/sec: 142.982\n",
      "INFO:tensorflow:global_step/sec: 138.819\n",
      "INFO:tensorflow:global_step/sec: 136.301\n",
      "INFO:tensorflow:global_step/sec: 130.383\n",
      "INFO:tensorflow:global_step/sec: 136.83\n",
      "Epoch 19/30 - loss: 0.2273 - acc: 0.9186\n",
      "INFO:tensorflow:global_step/sec: 97.2452\n",
      "INFO:tensorflow:global_step/sec: 130.688\n",
      "INFO:tensorflow:global_step/sec: 139.848\n",
      "INFO:tensorflow:global_step/sec: 138.922\n",
      "INFO:tensorflow:global_step/sec: 137.073\n",
      "INFO:tensorflow:global_step/sec: 115.497\n",
      "INFO:tensorflow:global_step/sec: 102.812\n",
      "INFO:tensorflow:global_step/sec: 105.319\n",
      "INFO:tensorflow:global_step/sec: 114.135\n",
      "Epoch 20/30 - loss: 0.2243 - acc: 0.9171\n",
      "INFO:tensorflow:global_step/sec: 84.9487\n",
      "INFO:tensorflow:global_step/sec: 72.2043\n",
      "INFO:tensorflow:global_step/sec: 125.221\n",
      "INFO:tensorflow:global_step/sec: 124.043\n",
      "INFO:tensorflow:global_step/sec: 120.764\n",
      "INFO:tensorflow:global_step/sec: 126.724\n",
      "INFO:tensorflow:global_step/sec: 122.14\n",
      "INFO:tensorflow:global_step/sec: 73.8473\n",
      "INFO:tensorflow:global_step/sec: 69.1914\n",
      "Epoch 21/30 - loss: 0.2175 - acc: 0.9202\n",
      "INFO:tensorflow:global_step/sec: 57.0061\n",
      "INFO:tensorflow:global_step/sec: 89.7251\n",
      "INFO:tensorflow:global_step/sec: 121.359\n",
      "INFO:tensorflow:global_step/sec: 133.553\n",
      "INFO:tensorflow:global_step/sec: 122.824\n",
      "INFO:tensorflow:global_step/sec: 144.025\n",
      "INFO:tensorflow:global_step/sec: 145.417\n",
      "INFO:tensorflow:global_step/sec: 145.358\n",
      "INFO:tensorflow:global_step/sec: 148.623\n",
      "INFO:tensorflow:global_step/sec: 138.152\n",
      "Epoch 22/30 - loss: 0.2153 - acc: 0.9211\n",
      "INFO:tensorflow:global_step/sec: 110.12\n",
      "INFO:tensorflow:global_step/sec: 148.19\n",
      "INFO:tensorflow:global_step/sec: 147.909\n",
      "INFO:tensorflow:global_step/sec: 115.982\n",
      "INFO:tensorflow:global_step/sec: 114.123\n",
      "INFO:tensorflow:global_step/sec: 135.422\n",
      "INFO:tensorflow:global_step/sec: 127.785\n",
      "INFO:tensorflow:global_step/sec: 116.575\n",
      "INFO:tensorflow:global_step/sec: 114.177\n",
      "Epoch 23/30 - loss: 0.2091 - acc: 0.9230\n",
      "INFO:tensorflow:global_step/sec: 68.6951\n",
      "INFO:tensorflow:global_step/sec: 86.6085\n",
      "INFO:tensorflow:global_step/sec: 119.065\n",
      "INFO:tensorflow:global_step/sec: 131.146\n",
      "INFO:tensorflow:global_step/sec: 124.986\n",
      "INFO:tensorflow:global_step/sec: 132.372\n",
      "INFO:tensorflow:global_step/sec: 136.318\n",
      "INFO:tensorflow:global_step/sec: 137.237\n",
      "INFO:tensorflow:global_step/sec: 125.658\n",
      "Epoch 24/30 - loss: 0.2055 - acc: 0.9237\n",
      "INFO:tensorflow:global_step/sec: 62.3878\n",
      "INFO:tensorflow:global_step/sec: 66.8754\n",
      "INFO:tensorflow:global_step/sec: 86.5392\n",
      "INFO:tensorflow:global_step/sec: 75.4765\n",
      "INFO:tensorflow:global_step/sec: 85.8135\n",
      "INFO:tensorflow:global_step/sec: 100.631\n",
      "INFO:tensorflow:global_step/sec: 112.082\n",
      "INFO:tensorflow:global_step/sec: 123.701\n",
      "INFO:tensorflow:global_step/sec: 134.374\n",
      "INFO:tensorflow:global_step/sec: 137.146\n",
      "Epoch 25/30 - loss: 0.2021 - acc: 0.9260\n",
      "INFO:tensorflow:global_step/sec: 104.655\n",
      "INFO:tensorflow:global_step/sec: 143.718\n",
      "INFO:tensorflow:global_step/sec: 133.222\n",
      "INFO:tensorflow:global_step/sec: 143.561\n",
      "INFO:tensorflow:global_step/sec: 136.087\n",
      "INFO:tensorflow:global_step/sec: 142.48\n",
      "INFO:tensorflow:global_step/sec: 144.034\n",
      "INFO:tensorflow:global_step/sec: 142.994\n",
      "INFO:tensorflow:global_step/sec: 134.792\n",
      "Epoch 26/30 - loss: 0.1970 - acc: 0.9263\n",
      "INFO:tensorflow:global_step/sec: 104.156\n",
      "INFO:tensorflow:global_step/sec: 137.225\n",
      "INFO:tensorflow:global_step/sec: 137.104\n",
      "INFO:tensorflow:global_step/sec: 143.192\n",
      "INFO:tensorflow:global_step/sec: 143.743\n",
      "INFO:tensorflow:global_step/sec: 142.563\n",
      "INFO:tensorflow:global_step/sec: 143.79\n",
      "INFO:tensorflow:global_step/sec: 132.936\n",
      "INFO:tensorflow:global_step/sec: 135.777\n",
      "Epoch 27/30 - loss: 0.1937 - acc: 0.9292\n",
      "INFO:tensorflow:global_step/sec: 118.617\n",
      "INFO:tensorflow:global_step/sec: 103.33\n",
      "INFO:tensorflow:global_step/sec: 133.346\n",
      "INFO:tensorflow:global_step/sec: 128.281\n",
      "INFO:tensorflow:global_step/sec: 115.309\n",
      "INFO:tensorflow:global_step/sec: 101.066\n",
      "INFO:tensorflow:global_step/sec: 113.492\n",
      "INFO:tensorflow:global_step/sec: 117.94\n",
      "INFO:tensorflow:global_step/sec: 94.616\n",
      "INFO:tensorflow:global_step/sec: 90.3447\n",
      "Epoch 28/30 - loss: 0.1904 - acc: 0.9295\n",
      "INFO:tensorflow:global_step/sec: 80.2991\n",
      "INFO:tensorflow:global_step/sec: 106.973\n",
      "INFO:tensorflow:global_step/sec: 126.725\n",
      "INFO:tensorflow:global_step/sec: 122.93\n",
      "INFO:tensorflow:global_step/sec: 127.364\n",
      "INFO:tensorflow:global_step/sec: 101.163\n",
      "INFO:tensorflow:global_step/sec: 98.7778\n",
      "INFO:tensorflow:global_step/sec: 110.136\n",
      "INFO:tensorflow:global_step/sec: 109.491\n",
      "Epoch 29/30 - loss: 0.1868 - acc: 0.9324\n",
      "INFO:tensorflow:global_step/sec: 91.1551\n",
      "INFO:tensorflow:global_step/sec: 108.598\n",
      "INFO:tensorflow:global_step/sec: 104.036\n",
      "INFO:tensorflow:global_step/sec: 116.925\n",
      "INFO:tensorflow:global_step/sec: 128.955\n",
      "INFO:tensorflow:global_step/sec: 88.8784\n",
      "INFO:tensorflow:global_step/sec: 98.5817\n",
      "INFO:tensorflow:global_step/sec: 105.46\n",
      "INFO:tensorflow:global_step/sec: 117.429\n",
      "INFO:tensorflow:global_step/sec: 108.398\n",
      "Epoch 30/30 - loss: 0.1853 - acc: 0.9322\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('monitored_session'):\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "            master=server.target,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            hooks=[_LoggerHook(), _InitHook(), _SaverHook(), model_average_hook],\n",
    "            config=sess_config,\n",
    "            stop_grace_period_secs=10,\n",
    "            save_checkpoint_secs=None) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_chief:\n",
    "    print('--- Begin Evaluation ---')\n",
    "    # Reset graph and load it again to clean tensors placed in other devices\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path + '.meta', clear_devices=True)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Model restored')\n",
    "        graph = tf.get_default_graph()\n",
    "        images_placeholder = graph.get_tensor_by_name('dataset/images_placeholder:0')\n",
    "        labels_placeholder = graph.get_tensor_by_name('dataset/labels_placeholder:0')\n",
    "        batch_size = graph.get_tensor_by_name('dataset/batch_size:0')\n",
    "        shuffle_size = graph.get_tensor_by_name('dataset/shuffle_size:0')\n",
    "        accuracy = graph.get_tensor_by_name('accuracy/accuracy_metric:0')\n",
    "        predictions = graph.get_tensor_by_name('softmax/BiasAdd:0')\n",
    "        dataset_init_op = graph.get_operation_by_name('dataset/dataset_init')\n",
    "        sess.run(dataset_init_op, feed_dict={images_placeholder: test_images, labels_placeholder: test_labels, batch_size: test_images.shape[0], shuffle_size: 1})\n",
    "        print('Test accuracy: {:4f}'.format(sess.run(accuracy)))\n",
    "        predicted = sess.run(predictions)\n",
    "\n",
    "    # Plot the first 25 test images, their predicted label, and the true label\n",
    "    # Color correct predictions in green, incorrect predictions in red\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "        predicted_label = np.argmax(predicted[i])\n",
    "        true_label = test_labels[i]\n",
    "        if predicted_label == true_label:\n",
    "          color = 'green'\n",
    "        else:\n",
    "          color = 'red'\n",
    "        plt.xlabel(\"{} ({})\".format(class_names[predicted_label],\n",
    "                                    class_names[true_label]),\n",
    "                                    color=color)\n",
    "\n",
    "    plt.show(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
