{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic distributed classifier with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is copyright 2018 <a href='https://comind.org/'>coMind</a>. Licensed under the Apache License, Version 2.0; you may not use this code except in compliance with the License. You may obtain a copy of the <a href='http://www.apache.org/licenses/LICENSE-2.0'>License</a>.\n",
    "\n",
    "Join the <a href='https://comindorg.slack.com/join/shared_invite/enQtNDMxMzc0NDA5OTEwLWIyZTg5MTg1MTM4NjhiNDM4YTU1OTI1NTgwY2NkNzZjYWY1NmI0ZjIyNWJiMTNkZmRhZDg2Nzc3YTYyNGQzM2I'>conversation</a> at Slack.\n",
    "\n",
    "This a series of three tutorials you are in the second one: \n",
    "* [Basic Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Classifier.ipynb)\n",
    "* [Basic Distributed Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Distributed%20Classifier.ipynb)\n",
    "* [Basic Federated Classifier](https://github.com/coMindOrg/federated-averaging-tutorials/blob/master/Basic%20Federated%20Classifier.ipynb)\n",
    "\n",
    "In this tutorial we will see what modifications we would have to make to the code on the <b>\"Basic Classifier\"</b> tutorial to be able to train it in a distributed way. Usually we would like to make the distribution among several different devices. As we will see later, this time we are going to execute everything in the same computer, hence instead of having the IP addresses of other devices we always have localhost:port. This way we simulate having several devices when really what we are doing is to launch different threads of execution in different ports of the same device.\n",
    "\n",
    "For those who are not familiar with TensorFlow we recommend taking a look first at the <b>\"Basic Classifier\"</b> tutorial.\n",
    "\n",
    "We start, importing the libraries that we will need and defining some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal with this script is to have 3 \"devices\" running in parallel. A __parameter server__ that is responsible for saving the global variables and 2 __workers__ who are, in short, those who perform the training operations.\n",
    "\n",
    "Using these flags is simply a way we can pass externally to this script the first variables that we are going to initialize. These are:\n",
    "\n",
    "- __job_name__: a string that is \"ps\" or \"worker\"\n",
    "- __task_index__: an integer. Since there will be several workers and you can also have several parameter servers, we have to give them a number to distinguish them.\n",
    "- __ps_hosts__: string with the IP addresses of the parameters servers\n",
    "- __worker_hosts__: string with the IP addresses of the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"task_index\", None,\n",
    "                     \"Worker task index, should be >= 0. task_index=0 is \"\n",
    "                     \"the master worker task that performs the variable \"\n",
    "                     \"initialization \")\n",
    "flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"job_name\", None, \"job name: worker or ps\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.job_name = 'worker'\n",
    "FLAGS.task_index = 1\n",
    "if FLAGS.job_name is None or FLAGS.job_name == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `job_name`\")\n",
    "if FLAGS.task_index is None or FLAGS.task_index == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `task_index`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we are going to use this code with a parameter server and two workers. So we will launch this script 3 times. One with values __job_name = ps, task_index = 0__; another __job_name = worker, task_index = 0__ and last, __job_name = worker, task_index = 1__.\n",
    "\n",
    "As long as we are working on the same device, we are going to disable the GPU to the 2 scripts of task_index = 0, because if the 3 of them try to access it at once, they will fill the memory.\n",
    "\n",
    "Then, we create lists of parameters servers and workers separating the strings of IP addresses, and we count the number of workers we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['localhost:2222']\n",
      "['localhost:2223', 'localhost:2224']\n",
      "2 workers defined\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.task_index == 0:\n",
    "    print('--- GPU Disabled ---')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "#Construct the cluster and start the server\n",
    "ps_spec = FLAGS.ps_hosts.split(\",\")\n",
    "worker_spec = FLAGS.worker_hosts.split(\",\")\n",
    "print(ps_spec)\n",
    "print(worker_spec)\n",
    "# Get the number of workers.\n",
    "num_workers = len(worker_spec)\n",
    "print('{} workers defined'.format(num_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the cluster, with the TensorFlow class <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec\"><b>tf.train.ClusterSpec</b></a>. As an initialization parameter, it receives a dictionary with the ps and worker keys, which index the lists of IP addresses created in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster we have a map of the network, with all the IPs. Now let's initialize the current server by telling it, with the job_name and the task_index, which of the cluster's devices it is the device that is running the current script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this script is running the parameter server, it will stay listening, waiting for the rest to write or read from it. The parameter server script will block here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.job_name == \"ps\":\n",
    "    print('--- Parameter Server Ready ---')\n",
    "    server.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything comming from here on will only be executed by the workers.\n",
    "\n",
    "We load the database as in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print('Data loaded')\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the database between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dataset size: 30000\n"
     ]
    }
   ],
   "source": [
    "train_images = np.split(train_images, num_workers)[FLAGS.task_index]\n",
    "train_labels = np.split(train_labels, num_workers)[FLAGS.task_index]\n",
    "print('Local dataset size: {}'.format(train_images.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed classifier works in the following way. Each of the workers trains a batch locally, and calculates the gradient that it would need to apply to its weights to reduce the cost. Then it writes the gradient in the parameter server. Once all the workers (or if there are too many, the minimum number that we have specified) have written their gradients in the parameters server, one of them, that acts as chief, reads all the gradients, averages them and updates the shared model stored in the ps. Finally the chief sends a signal to the rest of the workers so that they can pull the latest model and keep training.\n",
    "\n",
    "\n",
    "One of the workers has to be the boss therefore, and we have to let it know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "is_chief = (FLAGS.task_index == 0)\n",
    "print(is_chief)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous tutorial we define here also the directory where we want to save the checkpoints, the summary etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: logs_dir/1582487205.9155247\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir='logs_dir/{}'.format(time())\n",
    "print('Checkpoint directory: ' + checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the name of the current device according to the criteria set by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker device: /job:worker/task:1 - is_chief: False\n"
     ]
    }
   ],
   "source": [
    "worker_device = \"/job:worker/task:%d\" % FLAGS.task_index\n",
    "print('Worker device: ' + worker_device + ' - is_chief: {}'.format(is_chief))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we will make the definition of the graph and the training in a way that is practically the same as in the __\"Basic Classifier\"__ tutorial.\n",
    "\n",
    "The first difference is seen in the __*with*__ that includes all the rest of the code. __tf.device__ creates a device-type object and __replica_device_setter__ takes care of the synchronization between devices to assign each operation to one of them (ps, workers, chief).\n",
    "\n",
    "The rest of the lines are all the same as the previous tutorial. The first change comes at the moment of defining the training and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-71e0b86625be>:19: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "WARNING:tensorflow:From <ipython-input-13-71e0b86625be>:19: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
      "WARNING:tensorflow:From <ipython-input-13-71e0b86625be>:23: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-71e0b86625be>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-71e0b86625be>:50: SyncReplicasOptimizer.__init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `SyncReplicaOptimizer` class is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\n",
      "INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2\n",
      "WARNING:tensorflow:From /home/absinthe/anaconda3/envs/cs7210spnew/lib/python3.7/site-packages/tensorflow_core/python/training/sync_replicas_optimizer.py:351: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Graph definition finished\n",
      "Training 2340 batches...\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, relu/kernel, relu/bias, softmax/kernel, softmax/bias, loss/Mean/ExponentialMovingAverage, accuracy/accuracy_metric/ExponentialMovingAverage, beta1_power, beta2_power, relu/kernel/Adam, relu/kernel/Adam_1, relu/bias/Adam, relu/bias/Adam_1, softmax/kernel/Adam, softmax/kernel/Adam_1, softmax/bias/Adam, softmax/bias/Adam_1, ready: None\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\n",
    "      tf.train.replica_device_setter(\n",
    "          worker_device=worker_device,\n",
    "          cluster=cluster)):\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    with tf.name_scope('dataset'), tf.device('/cpu:0'):\n",
    "        images_placeholder = tf.placeholder(train_images.dtype, [None, train_images.shape[1], train_images.shape[2]], \n",
    "                                            name='images_placeholder')\n",
    "        labels_placeholder = tf.placeholder(train_labels.dtype, [None], name='labels_placeholder')\n",
    "        batch_size = tf.placeholder(tf.int64, name='batch_size')\n",
    "        shuffle_size = tf.placeholder(tf.int64, name='shuffle_size')\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images_placeholder, labels_placeholder))\n",
    "        dataset = dataset.shuffle(shuffle_size, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat(EPOCHS)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n",
    "        X, y = iterator.get_next()\n",
    "\n",
    "    flatten_layer = tf.layers.flatten(X, name='flatten')\n",
    "\n",
    "    dense_layer = tf.layers.dense(flatten_layer, 128, activation=tf.nn.relu, name='relu')\n",
    "\n",
    "    predictions = tf.layers.dense(dense_layer, 10, activation=tf.nn.softmax, name='softmax')\n",
    "\n",
    "    summary_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(keras.losses.sparse_categorical_crossentropy(y, predictions))\n",
    "        loss_averages_op = summary_averages.apply([loss])\n",
    "        tf.summary.scalar('cross_entropy', summary_averages.average(loss))\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.cast(y, tf.int64))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy_metric')\n",
    "        accuracy_averages_op = summary_averages.apply([accuracy])\n",
    "        tf.summary.scalar('accuracy', summary_averages.average(accuracy))\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        \n",
    "        # SynReplicasOptimizer is a wrapper for the optimizer.\n",
    "        # It is responsible for synchronizing the optimizers of all workers, \n",
    "        # wait for a certain number of them (replicas_to_aggregate)\n",
    "        # to have written their gradients in the parameter server and calculates the averages of those gradients.\n",
    "        optimizer = tf.train.SyncReplicasOptimizer(tf.train.AdamOptimizer(np.sqrt(num_workers) * 0.001), \n",
    "                                                   replicas_to_aggregate=num_workers)\n",
    "        with tf.control_dependencies([loss_averages_op, accuracy_averages_op]):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        # This hook is responsible for creating and initializing variables and operations for SyncReplicasOptimizer.\n",
    "        sync_replicas_hook = optimizer.make_session_run_hook(is_chief)\n",
    "\n",
    "    print('Graph definition finished')\n",
    "\n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False,\n",
    "        device_filters=[\"/job:ps\",\n",
    "        \"/job:worker/task:%d\" % FLAGS.task_index])\n",
    "\n",
    "    n_batches = int(train_images.shape[0] / (BATCH_SIZE * num_workers))\n",
    "    last_step = int(n_batches * EPOCHS)\n",
    "\n",
    "    print('Training {} batches...'.format(last_step))\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "        def begin(self):\n",
    "            self._total_loss = 0\n",
    "            self._total_acc = 0\n",
    "\n",
    "        def before_run(self, run_context):\n",
    "            return tf.train.SessionRunArgs([loss, accuracy, global_step])\n",
    "\n",
    "        def after_run(self, run_context, run_values):\n",
    "            loss_value, acc_value, step_value = run_values.results\n",
    "            self._total_loss += loss_value\n",
    "            self._total_acc += acc_value\n",
    "            if (step_value + 1) % n_batches == 0 and not step_value == 0:\n",
    "                print(\"Epoch {}/{} - loss: {:.4f} - acc: {:.4f}\".format(\n",
    "                    int(step_value / n_batches) + 1, EPOCHS, self._total_loss / n_batches, self._total_acc / n_batches))\n",
    "                self._total_loss = 0\n",
    "                self._total_acc = 0\n",
    "\n",
    "        def end(self, session):\n",
    "            print(\"Epoch {}/{} - loss: {:.4f} - acc: {:.4f}\".format(\n",
    "                int(session.run(global_step) / n_batches) + 1, EPOCHS, self._total_loss / n_batches, self._total_acc / n_batches))\n",
    "\n",
    "    class _InitHook(tf.train.SessionRunHook):\n",
    "        def after_create_session(self, session, coord):\n",
    "            session.run(dataset_init_op, feed_dict={\n",
    "                images_placeholder: train_images, labels_placeholder: train_labels, \n",
    "                batch_size: BATCH_SIZE, shuffle_size: train_images.shape[0]})\n",
    "    # The last difference comes in the definition of the monitored session.\n",
    "    # With the first argument we let it know with which server we will be working during the session.\n",
    "    # Then we tell him if the current script is the chief or not.\n",
    "    # stop_grace_period_secs causes the session to end 10 seconds after the operations are finished\n",
    "    with tf.name_scope('monitored_session'):\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "                master=server.target,\n",
    "                is_chief=is_chief,\n",
    "                checkpoint_dir=checkpoint_dir,\n",
    "                hooks=[_LoggerHook(), _InitHook(), sync_replicas_hook],\n",
    "                config=sess_config,\n",
    "                stop_grace_period_secs=10,\n",
    "                save_checkpoint_steps=n_batches) as mon_sess:\n",
    "            while not mon_sess.should_stop():\n",
    "                mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model we will do the same as before. Load the checkpoints, since our model does not exist once the session is closed.\n",
    "\n",
    "In this case the difference is that we are going to restart the graph completely and reload it, so in addition to loading the operations we need (accuracy and prediction), we have to load the placeholders and the initialization operation of the iterator. This is due to the fact that the original graph had tensors placed in the parameter server which we don't need for the inference operations.\n",
    "\n",
    "Finally we draw the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_chief:\n",
    "    print('--- Begin Evaluation ---')\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path + '.meta', clear_devices=True)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Model restored')\n",
    "        graph = tf.get_default_graph()\n",
    "        images_placeholder = graph.get_tensor_by_name('dataset/images_placeholder:0')\n",
    "        labels_placeholder = graph.get_tensor_by_name('dataset/labels_placeholder:0')\n",
    "        batch_size = graph.get_tensor_by_name('dataset/batch_size:0')\n",
    "        accuracy = graph.get_tensor_by_name('accuracy/accuracy_metric:0')\n",
    "        predictions = graph.get_tensor_by_name('softmax/BiasAdd:0')\n",
    "        dataset_init_op = graph.get_operation_by_name('dataset/dataset_init')\n",
    "        sess.run(dataset_init_op, feed_dict={\n",
    "            images_placeholder: test_images, labels_placeholder: test_labels, \n",
    "            batch_size: test_images.shape[0], shuffle_size: 1})\n",
    "        print('Test accuracy: {:4f}'.format(sess.run(accuracy)))\n",
    "        predicted = sess.run(predictions)\n",
    "        \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "        predicted_label = np.argmax(predicted[i])\n",
    "        true_label = test_labels[i]\n",
    "        if predicted_label == true_label:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.xlabel(\"{} ({})\".format(class_names[predicted_label],\n",
    "                                    class_names[true_label]),\n",
    "                                    color=color)\n",
    "\n",
    "    plt.show(True)          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
